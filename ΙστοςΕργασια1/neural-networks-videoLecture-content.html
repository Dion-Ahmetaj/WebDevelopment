<!DOCTYPE html>
<html lang = "el">
<head>
    <meta charset = "UTF-8">
    <meta name = "viewport" content = "width=device-width, initial-scale=1.0">
    <title>Εισαγωγή στα Νευρωνικά Δίκτυα</title>
    <link rel = "stylesheet" href = "styles.css">
</head>
<body>

    <div class = "grid container">

    <header>
        <h1>But what is a neural network? | Deep learning chapter 1</h1>
        <p>3Blue1Brown, 2017</p>
        <p>Αυτή η σειρά βίντεο προσφέρει μια ολοκληρωμένη εισαγωγή στις βασικές αρχές και τις σύγχρονες εφαρμογές των νευρωνικών δικτύων και της τεχνητής νοημοσύνης. Ξεκινώντας με μια γενική εξήγηση του τι είναι ένα νευρωνικό δίκτυο και πώς μαθαίνει, η σειρά εμβαθύνει σε κρίσιμες έννοιες όπως η Gradient Descent, ο τρόπος με τον οποίο τα νευρωνικά δίκτυα προσαρμόζουν τα βάρη τους μέσω της μάθησης, και η Backpropagation, η οποία αποτελεί τη βάση για την εκπαίδευση των μοντέλων.</p>
        <p>★★★★★ (437.992 αξιολογήσεις)</p>
        <p><a href = "neural-networks.html">Επιστροφή στην υποκατηγορία εκπαιδευτικού υλικού</a></p>
    </header>

    <nav id = "toc">
        <h2>Πίνακας Περιεχομένων</h2>
        <ul>
            <li><a href="#section1">But what is a neural network? | Deep learning chapter 1</a></li>
            <li><a href="#section2">Gradient descent, how neural networks learn | DL2</a></li>
            <li><a href="#section3">Backpropagation, step-by-step | DL3</a></li>
            <li><a href="#section4">Backpropagation calculus | DL4</a></li>
            <li><a href="#section5">Transformers (how LLMs work) explained visually | DL5</a></li>
            <li><a href="#section6">Attention in transformers, visually explained | DL6</a></li>
            <li><a href="#section7">How might LLMs store facts | DL7</a></li>
        </ul>
    </nav>

    <main>
        <section id = "section1">
            <h2>Κεφάλαιο 1: Introduction</h2>
            <p>This chapter is an introduction to neural networks, explaining how they simulate the functioning of the human brain. It introduces the basic principles, such as the learning process through examples, and shows how networks adjust weights to make correct decisions.</p>
            <iframe width = "560" height = "315" src = "https://www.youtube.com/embed/aircAruvnKk?si=f_k1YgiThCxwyVaU" frameborder = "0" allowfullscreen></iframe>
            <p><a href = "#toc">Επιστροφή στον Πίνακα Περιεχομένων</a></p>
        </section>

        <section id = "section2">
            <h2>Κεφάλαιο 2: Gradient descent</h2>
            <p>In this chapter we dive into the Gradient Descent method, a fundamental algorithm for optimization in neural networks. It shows how network weights are adjusted to minimize prediction error, giving a practical understanding of how networks "learn".</p>
            <iframe width = "560" height="315" src = "https://www.youtube.com/embed/IHZwWFHWa-w?si=SCcqixlaWaSzH5bs" frameborder = "0" allowfullscreen></iframe>
            <p><a href = "#toc">Επιστροφή στον Πίνακα Περιεχομένων</a></p>
        </section>

        <section id = "section3">
            <h2>Κεφάλαιο 3: Backpropagation, step-by-step</h2>
            <p>This chapter explains the process of backpropagation, showing how derivatives are used to optimize weights in neural networks. With clear and visual examples, it highlights the role of optimization through weight adaptation.</p>
            <iframe width = "560" height = "315" src = "https://www.youtube.com/embed/Ilg3gGewQ5U?si=kS6Gc1vyaVVtbXnI" frameborder = "0" allowfullscreen></iframe>
            <p><a href = "#toc">Επιστροφή στον Πίνακα Περιεχομένων</a></p>
        </section>

        <section id = "section4">
            <h2>Κεφάλαιο 4: Backpropagation calculus</h2>
            <p>In this chapter we take a look at the mathematical aspects of backpropagation, explaining the use of derivatives to optimize neural networks via Gradient Descent.</p>
            <iframe width = "560" height = "315" src = "https://www.youtube.com/embed/tIeHLnjs5U8?si=8NxCFuGvnZTyIGA5" frameborder = "0" allowfullscreen></iframe>
            <p><a href = "#toc">Επιστροφή στον Πίνακα Περιεχομένων</a></p>
        </section>

        <section id = "section5">
            <h2>Κεφάλαιο 5: Transformers (how LLMs work) explained visually</h2>
            <p>The chapter visually explains the operation of Transformers, revealing how these models form the basis of Large Language Models, focusing on the attention mechanism and model structure.</p>
            <iframe width = "560" height = "315" src = "https://www.youtube.com/embed/wjZofJX0v4M?si=-tuGooDEBBgbeFwg" frameborder = "0" allowfullscreen></iframe>
            <p><a href = "#toc">Επιστροφή στον Πίνακα Περιεχομένων</a></p>
        </section>

        <section id = "section6">
            <h2>Κεφάλαιο 6: Attention in transformers, visually explained</h2>
            <p>In this chapter, I explain the attention mechanism in Transformers, showing how words influence and connect with each other, enabling efficient processing of large texts.</p>
            <iframe width = "560" height = "315" src = "https://www.youtube.com/embed/eMlx5fFNoYc?si=zkV4nLv6K4y56egd" frameborder = "0" allowfullscreen></iframe>
            <p><a href = "#toc">Επιστροφή στον Πίνακα Περιεχομένων</a></p>
        </section>

        <section id = "section7">
            <h2>Κεφάλαιο 7: How might LLMs store facts</h2>
            <p>In the final chapter, we examine how Large Language Models (LLMs) store and retrieve knowledge, analysing how they organise information and exploit it to extract facts.</p>
            <iframe width = "560" height = "315" src = "https://www.youtube.com/embed/9-Jl0dxWQs8?si=Xkiz5o_Bi-WrjlHo" frameborder = "0" allowfullscreen></iframe>
            <p><a href = "#toc">Επιστροφή στον Πίνακα Περιεχομένων</a></p>
        </section>
    </main>

    <footer class = "footer">
        <p>Επικοινωνία: info@elearning-service.gr</p>
    </footer>

    </div>

</body>
</html>